---
title: "Data Acquisition with APIs in R"
format:
  pdf: default
editor_options: 
  chunk_output_type: console
---
  
You can download this .qmd file from [here](https://github.com/joeroith/264_spring_2025/blob/main/07_apis.qmd).  Just hit the Download Raw File button.

Credit to Brianna Heggeseth and Leslie Myint from Macalester College for a few of these descriptions and examples.

```{r}
#| include: FALSE

library(tidyverse)
library(stringr)
library(httr2)
library(httr)
```


## Getting data from websites

# Option 1: APIs

When we interact with sites like The New York Times, Zillow, and Google, we are accessing their data via a graphical layout (e.g., images, colors, columns) that is easy for humans to read but hard for computers.

An **API** stands for **Application Programming Interface**, and this term describes a general class of tool that allows computers, rather than humans, to interact with an organization's data. How does this work?

- When we use web browsers to navigate the web, our browsers communicate with web servers using a technology called [HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) or Hypertext Transfer Protocol to get information that is formatted into the display of a web page.
- Programming languages such as R can also use HTTP to communicate with web servers. The easiest way to do this is via [Web APIs](https://en.wikipedia.org/wiki/Web_API), or Web Application Programming Interfaces, which focus on transmitting raw data, rather than images, colors, or other appearance-related information that humans interact with when viewing a web page.

A large variety of web APIs provide data accessible to programs written in R (and almost any other programming language!). Almost all reasonably large commercial websites offer APIs. Todd Motto has compiled an expansive list of [Public Web APIs](https://github.com/toddmotto/public-apis) on GitHub, although it's about 3 years old now so it's not a perfect or complete list. Feel free to browse this list to see what data sources are available.

For our purposes of obtaining data, APIs exist where website developers make data nicely packaged for consumption.  The language HTTP (hypertext transfer protocol) underlies APIs, and the R package `httr()` (and now the updated `httr2()`) was written to map closely to HTTP with R. Essentially you send a request to the website (server) where you want data from, and they send a response, which should contain the data (plus other stuff).

The case studies in this document provide a really quick introduction to data acquisition, just to get you started and show you what's possible.  For more information, these links can be somewhat helpful:

- https://www.geeksforgeeks.org/functions-with-r-and-rvest/#
- https://nceas.github.io/oss-lessons/data-liberation/intro-webscraping.html


## Wrapper packages

In R, it is easiest to use Web APIs through a **wrapper package**, an R package written specifically for a particular Web API.

- The R development community has already contributed wrapper packages for many large Web APIs (e.g. ZillowR, rtweet, genius, Rspotify, tidycensus, etc.)
- To find a wrapper package, search the web for "R package" and the name of the website. For example:
    - Searching for "R Reddit package" returns [RedditExtractor](https://github.com/ivan-rivera/RedditExtractor)
    - Searching for "R Weather.com package" returns [weatherData](https://ram-n.github.io/weatherData/)
- [rOpenSci](https://ropensci.org/packages/) also has a good collection of wrapper packages.

In particular, `tidycensus` is a wrapper package that makes it easy to obtain desired census information for mapping and modeling:

```{r}
#| include = FALSE

# Grab total population and median household income for all
#   census tracts in MN using Census Bureau API
library(tidycensus)
sample_acs_data <- tidycensus::get_acs(
    year = 2020,
    state = "MN",
    geography = "tract",
    variables = c("B01003_001", "B19013_001"),
    output = "wide",
    geometry = TRUE
)
```

Obtaining raw data from the Census Bureau was that easy!  Often we will have to obtain and use a secret API key to access the data, but that's not always necessary with `tidycensus`.

Now we can tidy that data and produce plots and analyses. [Here's a decent place to get more information about the variable codes.](https://api.census.gov/data/2019/acs/acs5/variables.html)

```{r}
#| warning: FALSE
#| message: FALSE

# Rename cryptic variables from the census form
sample_acs_data <- sample_acs_data |>
  rename(population = B01003_001E,
         population_moe = B01003_001M,
         median_income = B19013_001E,
         median_income_moe = B19013_001M)

# Plot with geom_sf since our data contains 1 row per census tract
#   with its geometry
ggplot(data = sample_acs_data) + 
  geom_sf(aes(fill = median_income), colour = "white", linetype = 2) + 
  theme_void()  

# The whole state of MN is overwhelming, so focus on a single county
sample_acs_data |>
  filter(str_detect(NAME, "Ramsey")) |>
  ggplot() + 
    geom_sf(aes(fill = median_income), colour = "white", linetype = 2)

# Look for relationships between variables with 1 row per tract
as_tibble(sample_acs_data) |>
  ggplot(aes(x = population, y = median_income)) + 
    geom_point() + 
    geom_smooth(method = "lm")  
```


Extra resources:

- `tidycensus`: wrapper package that provides an interface to a few census datasets *with map geometry included!*
    - Full documentation is available at <https://walker-data.com/tidycensus/>

- `censusapi`: wrapper package that offers an interface to all census datasets
    - Full documentation is available at <https://www.hrecht.com/censusapi/>

`get_acs()` is one of the functions that is part of `tidycensus`.  Let's explore what's going on behind the scenes with `get_acs()`...


## Accessing web APIs directly

### Getting a Census API key

Many APIs (and their wrapper packages) require users to obtain a **key** to use their services.

- This lets organizations keep track of what data is being used.
- It also **rate limits** their API and ensures programs don't make too many requests per day/minute/hour. Be aware that most APIs do have rate limits --- especially for their free tiers.

Navigate to <https://api.census.gov/data/key_signup.html> to obtain a Census API key:

- Organization: St. Olaf College
- Email: Your St. Olaf email address

You will get the message:

> Your request for a new API key has been successfully submitted. Please check your email. In a few minutes you should receive a message with instructions on how to activate your new key.

Check your email. Copy and paste your key into a new text file:

- (In RStudio) File > New File > Text File (towards the bottom of the menu)
- Save as `census_api_key.txt` in the same folder as this `.qmd`.

You could then read in the key with code like this:

```{r}
myapikey <- readLines("C:/Users/charl/Documents/SDS_264/census_api_key")
```


### Handling API keys

While this works, the problem is once we start backing up our files to GitHub, your API key will also appear on GitHub, and you want to keep your API key secret.  Thus, we might use **environment variables** instead:

One way to store a secret across sessions is with environment variables. Environment variables, or envvars for short, are a cross platform way of passing information to processes.  For passing envvars to R, you can list name-value pairs in a file called .Renviron in your home directory. The easiest way to edit it is to run:

```{r}
#| eval: FALSE

file.edit("~/.Renviron")

Sys.setenv(PATH = "path", VAR1 = "value1", VAR2 = "value2")
```

The file looks something like

PATH = "path"
VAR1 = "value1"
VAR2 = "value2"
And you can access the values in R using `Sys.getenv()`:

```{r}
#| eval: FALSE

Sys.getenv("VAR1")
#> [1] "value1"
```

Note that .Renviron is only processed on startup, so youâ€™ll need to restart R to see changes.

Another option is to use `Sys.setenv` and `Sys.getenv`:

```{r}
# I used the first line to store my CENSUS API key in .Renviron
#   after uncommenting - should only need to run one time
#Sys.setenv(CENSUS_API_KEY = "my personal key")
my_census_api_key <- Sys.getenv("CENSUS_API_KEY")
```


### Navigating API documentation

Navigate to the [Census API user guide](https://www.census.gov/data/developers/guidance/api-user-guide.html) and click on the "Example API Queries" tab.

Let's look at the Population Estimates Example and the American Community Survey (ACS) Example. These examples walk us through the steps to incrementally build up a URL to obtain desired data. This URL is known as a web API **request**. 

https://api.census.gov/data/2019/acs/acs1?get=NAME,B02015_009E,B02015_009M&for=state:*

- `https://api.census.gov`: This is the **base URL**.
    - `http://`: The **scheme**, which tells your browser or program how to communicate with the web server. This will typically be either `http:` or `https:`.
    - `api.census.gov`: The **hostname**, which is a name that identifies the web server that will process the request.
- `data/2019/acs/acs1`: The **path**, which tells the web server how to get to the desired resource.
    - In the case of the Census API, this locates a desired dataset in a particular year.
    - Other APIs allow search functionality. (e.g., News organizations have article searches.) In these cases, the path locates the search function we would like to call.
- `?get=NAME,B02015_009E,B02015_009M&for=state:*`: The **query parameters**, which provide the parameters for the function you would like to call.
    - We can view this as a string of key-value pairs separated by `&`. That is, the general structure of this part is `key1=value1&key2=value2`.

key      value
----     ------
get      NAME,B02015_009E,B02015_009M
for      state:*

Typically, each of these URL components will be specified in the API documentation. Sometimes, the scheme, hostname, and path (`https://api.census.gov/data/2019/acs/acs1`) will be referred to as the **[endpoint](https://en.wikipedia.org/wiki/Web_API#Endpoints)** for the API call.

We will first use the [`httr2` package](https://httr2.r-lib.org/) to build up a full URL from its parts.

- `request()` creates an API request object using the **base URL**
- `req_url_path_append()` builds up the URL by adding path components separated by `/`
- `req_url_query()` adds the `?` separating the endpoint from the query and sets the key-value pairs in the query
    - The `.multi` argument controls how multiple values for a given key are combined.
    - The `I()` function around `"state:*"` inhibits parsing of special characters like `:` and `*`. (It's known as the "as-is" function.)
    - The backticks around `for` are needed because `for` is a reserved word in R (for for-loops). You'll need backticks whenever the key name has special characters (like spaces, dashes).
    - We can see from [here](https://www.census.gov/data/developers/guidance/api-user-guide.Help_&_Contact_Us.html) that providing an API key is achieved with `key=YOUR_API_KEY`.


```{r}
# Request total number of Hmong residents and margin of error by state
#   in 2019, as in the User Guide
CENSUS_API_KEY <- Sys.getenv("CENSUS_API_KEY")
req <- request("https://api.census.gov") |> 
    req_url_path_append("data") |> 
    req_url_path_append("2019") |> 
    req_url_path_append("acs") |> 
    req_url_path_append("acs1") |> 
    req_url_query(get = c("NAME", "B02015_009E", "B02015_009M"), `for` = I("state:*"), key = CENSUS_API_KEY, .multi = "comma")
```

**Why would we ever use these steps instead of just using the full URL as a string?**

- To generalize this code with functions! (This is exactly what wrapper packages do.)
- To handle special characters
    - e.g., query parameters might have spaces, which need to be represented in a particular way in a URL (URLs can't contain spaces)

Once we've fully constructed our request, we can use `req_perform()` to send out the API request and get a **response**.

```{r}
#| message: FALSE

resp <- req_perform(req)
resp
```

We see from `Content-Type` that the format of the response is something called JSON. We can navigate to the request URL to see the structure of this output.

- JSON (Javascript Object Notation) is a nested structure of key-value pairs.
- We can use `resp_body_json()` to parse the JSON into a nicer format.
    - Without `simplifyVector = TRUE`, the JSON is read in as a list. 

```{r}
resp_json_list <- resp |> resp_body_json()
head(resp_json_list, 2)
resp_json_df <- resp |> resp_body_json(simplifyVector = TRUE)
head(resp_json_df)
resp_json_df <- janitor::row_to_names(resp_json_df, 1)
head(resp_json_df)
```


All right, let's try this!  First we'll grab total population and median household income for all census tracts in MN using 3 approaches

```{r}
#| eval: FALSE

# First using tidycenus
library(tidycensus)
sample_acs_data <- tidycensus::get_acs(
    year = 2021,
    state = "MN",
    geography = "tract",
    variables = c("B01003_001", "B19013_001"),
    output = "wide",
    geometry = TRUE,
    county = "Hennepin",   # specify county in call
    show_call = TRUE       # see resulting query
)
```

```{r}
#| message: FALSE

# Next using httr2
req <- request("https://api.census.gov") |> 
    req_url_path_append("data") |> 
    req_url_path_append("2020") |> 
    req_url_path_append("acs") |> 
    req_url_path_append("acs5") |> 
    req_url_query(get = c("NAME", "B01003_001E", "B19013_001E"), `for` = I("tract:*"), `in` = I("state:27"), `in` = I("county:053"), key = CENSUS_API_KEY, .multi = "comma")

resp <- req_perform(req)
resp
resp_json_df <- resp |> resp_body_json(simplifyVector = TRUE)
head(resp_json_df)
resp_json_df <- janitor::row_to_names(resp_json_df, 1)
head(resp_json_df)

hennepin_httr2 <- as_tibble(resp_json_df) |>
  mutate(population = parse_number(B01003_001E),
         median_income = parse_number(B19013_001E)) |>
  select(-B01003_001E, -B19013_001E, -state, -county)
  
hennepin_httr2 |>
  ggplot(aes(x = population, y = median_income)) + 
    geom_point() + 
    geom_smooth(method = "lm")  

summary(hennepin_httr2$population)
summary(hennepin_httr2$median_income)
sort(hennepin_httr2$population)
sort(hennepin_httr2$median_income)

hennepin_httr2 <- hennepin_httr2 |>
  mutate(median_income = ifelse(median_income > 0, median_income, NA),
         population = ifelse(population > 0, population, NA))
  
hennepin_httr2 |>
  ggplot(aes(x = population, y = median_income)) + 
    geom_point() + 
    geom_smooth(method = "lm")  

# To make choropleth map by census tract, would need to download US Census
#   Bureau TIGER geometries using tigris package
```

```{r}
#| message: FALSE

# Finally using httr
url <- str_c("https://api.census.gov/data/2020/acs/acs5?get=NAME,B01003_001E,B19013_001E&for=tract:*&in=state:27&in=county:053", "&key=", CENSUS_API_KEY)
acs5 <- GET(url)
details <- content(acs5, "parsed")
# details 
details[[1]]  # variable names
details[[2]]  # list with information on 1st tract

name = character()
population = double()
median_income = double()
tract = character()

for(i in 2:330) {
  name[i-1] <- details[[i]][[1]][1]
  population[i-1] <- details[[i]][[2]][1]
  median_income[i-1] <- details[[i]][[3]][1]
  tract[i-1] <- details[[i]][[6]][1]
}
hennepin_httr <- tibble(
  name = name,
  population = parse_number(population),
  median_income = parse_number(median_income),
  tract = tract
)
```


### On Your Own

1. Write a for loop to obtain the Hennepin County data from 2017-2021


2. Write a function to give choices about year, county, and variables


```{r}
# function to allow user inputs

MN_tract_data <- function(year, county, variables) {
  tidycensus::get_acs(
    Sys.sleep(0.5),
    year = year,
    state = "MN",
    geography = "tract",
    variables = variables,
    output = "wide",
    geometry = TRUE,
    county = county
  ) |>
    mutate(year = year)
}

# Should really build in checks so that county is in MN, year is in 
#   proper range, and variables are part of ACS1 data set

my_data <- MN_tract_data(year = 2021,
              county = "Hennepin", 
              variables = c("B01003_001", "B19013_001"))

ggplot(data = my_data) + 
  geom_sf(aes(fill = B01003_001E), colour = "white", linetype = 2)

my_data <- MN_tract_data(year = 2022,
              county = "Rice", 
              variables = c("B01003_001", "B19013_001"))

ggplot(data = my_data) + 
  geom_sf(aes(fill = B01003_001E), colour = "white", linetype = 2)

# Try other variables:
#  - B25077_001 is median home price
#  - B02001_002 is number of white residents
#  - etc.
# although the census codebook is admittedly quite daunting!
```

3. Use your function from (2) along with `map` and `list_rbind` to build a data set for Rice county for the years 2019-2021

```{r}
# To examine trends over time in Rice County
2019:2021 |>
  purrr::map(\(x) 
    MN_tract_data(
      x,
      county = "Rice", 
      variables = c("B01003_001", "B19013_001")
    )
  ) |>
  list_rbind()

# Or a little more simply
2019:2021 |>
  purrr::map(MN_tract_data,
             county = "Rice", 
             variables = c("B01003_001", "B19013_001")
            ) |>
  list_rbind()
```

### One more example using an API key

Here's an example of getting data from a website that attempts to make imdb movie data available as an API.

Initial instructions:

- go to omdbapi.com under the API Key tab and request a free API key
- store your key as discussed earlier
- explore the examples at omdbapi.com

We will first obtain data about the movie Coco from 2017.

```{r}

# I addeed: Sys.setenv(OMDB_KEY = "")
# I used the first line to store my OMDB API key in .Renviron
# Sys.setenv(OMDB_KEY = "paste my omdb key here")
myapikey <- Sys.getenv("OMDB_KEY")

# Find url exploring examples at omdbapi.com
url <- str_c("http://www.omdbapi.com/?t=Coco&y=2017&apikey=", myapikey)

coco <- GET(url)   # coco holds response from server
coco               # Status of 200 is good!

details <- content(coco, "parse")   
details                         # get a list of 25 pieces of information
details$Year                    # how to access details
details[[2]]                    # since a list, another way to access
```

Now build a data set for a collection of movies

```{r}
#| message: FALSE

# Must figure out pattern in URL for obtaining different movies
#  - try searching for others
movies <- c("Coco", "Wonder+Woman", "Get+Out", 
            "The+Greatest+Showman", "Thor:+Ragnarok")

# Set up empty tibble
omdb <- tibble(Title = character(), Rated = character(), Genre = character(),
       Actors = character(), Metascore = double(), imdbRating = double(),
       BoxOffice = double())

# Use for loop to run through API request process 5 times,
#   each time filling the next row in the tibble
#  - can do max of 1000 GETs per day
for(i in 1:5) {
  url <- str_c("http://www.omdbapi.com/?t=",movies[i],
               "&apikey=", myapikey)
  Sys.sleep(0.5)
  onemovie <- GET(url)
  details <- content(onemovie, "parse")
  omdb[i,1] <- details$Title
  omdb[i,2] <- details$Rated
  omdb[i,3] <- details$Genre
  omdb[i,4] <- details$Actors
  omdb[i,5] <- parse_number(details$Metascore)
  omdb[i,6] <- parse_number(details$imdbRating)
  omdb[i,7] <- parse_number(details$BoxOffice)   # no $ and ,'s
}

omdb

#  could use stringr functions to further organize this data - separate 
#    different genres, different actors, etc.
```

```{r}
movies <- c("Up", "Cars", "Kung+Fu+Panda", "The+Emperor%27s+New+Groove", "Mulan")

omdb <- tibble(Title = character(), Released = character(), Runtime = character(), Plot = character(), BoxOffice = double())

for(i in 1:5) {
  url <- str_c("http://www.omdbapi.com/?t=",movies[i],
               "&apikey=", myapikey)
  Sys.sleep(0.5)
  onemovie <- GET(url)
  details <- content(onemovie, "parse")
  omdb[i,1] <- details$Title
  omdb[i,2] <- details$Released
  omdb[i,3] <- details$Runtime
  omdb[i,4] <- details$Plot
  omdb[i,5] <- parse_number(details$BoxOffice)
}
```

### On Your Own (continued)

4. (Based on final project by Mary Wu and Jenna Graff, MSCS 264, Spring 2024).  Start with a small data set on 56 national parks from [kaggle](https://www.kaggle.com/datasets/nationalparkservice/park-biodiversity), and supplement with columns for the park address (a single column including address, city, state, and zip code) and a list of available activities (a single character column with activities separated by commas) from the park websites themselves.

Preliminaries:

- Request API [here](https://www.nps.gov/subjects/developer/get-started.htm)
- Check out [API guide](https://www.nps.gov/subjects/developer/guides.htm)

```{r}
#| message: FALSE
#| eval: FALSE
np_kaggle <- read_csv("Data/parks.csv")
```
  
You can download this .qmd file from [here](https://github.com/joeroith/264_spring_2025/blob/main/08_table_scraping.qmd).  Just hit the Download Raw File button.


```{r}
#| include: FALSE

library(tidyverse)
library(stringr)
library(rvest)
library(polite)
library(sf)
library(maps)
library(viridis)
library(leaflet)
library(htmltools)

```


# Using rvest for web scraping

If you would like to assemble data from a website with no API, you can often acquire data using more brute force methods commonly called web scraping.  Typically, this involves finding content inside HTML (Hypertext markup language) code used for creating webpages and web applications and the CSS (Cascading style sheets) language for customizing the appearance of webpages. 
We are used to reading data from .csv files.... but most websites have it stored in XML (like html, but for data). You can read more about it here if you're interested: https://www.w3schools.com/xml/default.asp

XML has a sort of tree or graph-like structure... so we can identify information by which `node` it belongs to (`html_nodes`) and then convert the content into something we can use in R (`html_text` or `html_table`).


Here's one quick example of web scraping.  First check out the webpage https://www.cheese.com/by_type and then select Semi-Soft.  We can drill into the html code for this webpage and find and store specific information (like cheese names)

```{r}
session <- bow("https://www.cheese.com/by_type", force = TRUE)
result <- scrape(session, query=list(t="semi-soft", per_page=100)) |>
  html_node("#main-body") |> 
  html_nodes("h3") |> 
  html_text()
head(result)
#> [1] "3-Cheese Italian Blend"  "Abbaye de Citeaux"      
#> [3] "Abbaye du Mont des Cats" "Adelost"                
#> [5] "ADL Brick Cheese"        "Ailsa Craig"
```


## Four steps to scraping data with functions in the `rvest` library:

0. `robotstxt::paths_allowed()` Check if the website allows scraping, and then make sure we scrape "politely"
1. `read_html()`.  Input the URL containing the data and turn the html code into an XML file (another markup format that's easier to work with).
2. `html_nodes()`.  Extract specific nodes from the XML file by using the CSS path that leads to the content of interest. (use css="table" for tables.)
3. `html_text()`.  Extract content of interest from nodes.  Might also use `html_table()` etc.


## Data scraping ethics

Before scraping, we should always check first whether the website allows scraping.  We should also consider if there's any personal or confidential information, and we should be considerate to not overload the server we're scraping from.

[Chapter 24 in R4DS](https://r4ds.hadley.nz/webscraping#scraping-ethics-and-legalities) provides a nice overview of some of the important issues to consider.  A couple of highlights:

- be aware of terms of service, and, if available, the `robots.txt` file that some websites will publish to clarify what can and cannot be scraped and other constraints about scraping.
- use the [`polite` package](https://github.com/dmi3kno/polite) to scrape public, non-personal, and factual data in a respectful manner
- scrape with a good purpose and request only what you need; in particular, be extremely wary of personally identifiable information

See [this article](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01) for more perspective on the ethics of data scraping.


## When the data is already in table form:

In this example, we will scrape climate data from [this website](https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503)

The website already contains data in table form, so we use `html_nodes(. , css = "table")` and `html_table()`

```{r}
# check that scraping is allowed (Step 0)
robotstxt::paths_allowed("https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503")

# Step 1: read_html()
mpls <- read_html("https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503")

# 2: html_nodes()
tables <- html_nodes(mpls, css = "table") 
tables  # have to guesstimate which table contains climate info

# 3: html_table()
html_table(tables, header = TRUE, fill = TRUE)    # find the right table
mpls_data1 <- html_table(tables, header = TRUE, fill = TRUE)[[1]]  
mpls_data1
mpls_data2 <- html_table(tables, header = TRUE, fill = TRUE)[[2]]  
mpls_data2
```

Now we wrap the 4 steps above into the `bow` and `scrape` functions from the `polite` package:

```{r}
session <- bow("https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503", force = TRUE)

result <- scrape(session) |>
  html_nodes(css = "table") |> 
  html_table(header = TRUE, fill = TRUE)
mpls_data1 <- result[[1]]
mpls_data2 <- result[[2]]
```


Even after finding the correct tables, there may still be a lot of work to make it tidy!!!  

**[Pause to Ponder:]** What is each line of code doing below?

```{r}
bind_cols(mpls_data1, mpls_data2) |>
  as_tibble() |> 
  select(-`...8`) |>
  mutate(`...1` = str_extract(`...1`, "[^ ]+ [^ ]+ [^ ]+")) |>
  pivot_longer(cols = c(`JanJa`:`DecDe`), 
               names_to = "month", values_to = "weather") |>
  pivot_wider(names_from = `...1`, values_from = weather) |>
  mutate(month = str_sub(month, 1, 3))  |>
  rename(avg_high = "Average high in",
         avg_low = "Average low in")

# Probably want to rename the rest of the variables too!
```


### Leaflet mapping example with data in table form

Let's return to our example from `02_maps.qmd` where we recreated an [interactive choropleth map](https://rstudio.github.io/leaflet/articles/choropleths.html) of population densities by US state.  Recall how that plot was very suspicious?  The population density data that came with the state geometries from [our source](https://rstudio.github.io/leaflet/json/us-states.geojson) seemed incorrect. 

Let's see if we can use our new web scraping skills to scrape the correct population density data and repeat that plot!  Can we go out and find the real statewise population densities, create a tidy data frame, merge that with our state geometry shapefiles, and then regenerate our plot?

A quick wikipedia search yields [this webpage](https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States_by_population_density) with more reasonable population densities in a nice table format.  Let's see if we can grab this data using our 4 steps to `rvest`ing data!

```{r}
# check that scraping is allowed (Step 0)
robotstxt::paths_allowed("https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States_by_population_density")

# Step 1: read_html()
pop_dens <- read_html("https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States_by_population_density")

# 2: html_nodes()
tables <- html_nodes(pop_dens, css = "table") 
tables  # have to guesstimate which table contains our desired info

# 3: html_table()
html_table(tables, header = TRUE, fill = TRUE)    # find the right table
density_table <- html_table(tables, header = TRUE, fill = TRUE)[[1]]  
density_table


# Perform Steps 0-3 using the polite package
session <- bow("https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States_by_population_density", force = TRUE)

result <- scrape(session) |>
  html_nodes(css = "table") |> 
  html_table(header = TRUE, fill = TRUE)
density_table <- result[[1]]
density_table
```

Even after grabbing our table from wikipedia and setting it in a nice tibble format, there is still some cleaning to do before we can merge this with our state geometries:

```{r}
density_data <- density_table |>
  select(1, 2, 4, 5) |>
  filter(!row_number() == 1) |>
  rename(Land_area = `Land area`) |>
  mutate(state_name = str_to_lower(as.character(Location)),
         Density = parse_number(Density),
         Population = parse_number(Population),
         Land_area = parse_number(Land_area)) |>
  select(-Location)
density_data
```

As before, we get core geometry data to draw US states and then we'll make sure we can merge our new density data into the core files.

```{r}
#| message: false
#| warning: false

# Get info to draw US states for geom_polygon (connect the lat-long points)
states_polygon <- as_tibble(map_data("state")) |>
  select(region, group, order, lat, long)

# See what the state (region) levels look like in states_polygon
unique(states_polygon$region)


# Get info to draw US states for geom_sf and leaflet (simple features
#   object with multipolygon geometry column)
states_sf <- read_sf("https://rstudio.github.io/leaflet/json/us-states.geojson") |>
  select(name, geometry)

# See what the state (name) levels look like in states_sf
unique(states_sf$name)


# See what the state (state_name) levels look like in density_data
unique(density_data$state_name)   
# all lower case plus some extraneous rows
```


```{r}
# Make sure all keys have the same format before joining: all lower case

states_sf <- states_sf |>
  mutate(name = str_to_lower(name))
```


```{r}
# Now we can merge data sets together for the static and the interactive plots

# Merge with states_polygon (static)
density_polygon <- states_polygon |>
  left_join(density_data, by = c("region" = "state_name"))
density_polygon

# Looks like merge worked for 48 contiguous states plus DC
density_polygon |>
  group_by(region) |>
  summarise(mean = mean(Density)) |>
  print(n = Inf)

# Remove DC since such an outlier
density_polygon <- density_polygon |>
  filter(region != "district of columbia")


# Merge with states_sf (static or interactive)
density_sf <- states_sf |>
  left_join(density_data, by = c("name" = "state_name")) |>
  filter(!(name %in% c("alaska", "hawaii")))

# Looks like merge worked for 48 contiguous states plus DC and PR
class(density_sf)
print(density_sf, n = Inf)

# Remove DC and PR
density_sf <- density_sf |>
  filter(name != "district of columbia" & name != "puerto rico")
```


Numeric variable (static plot):

```{r}
density_polygon |>
  ggplot(mapping = aes(x = long, y = lat, group = group)) + 
    geom_polygon(aes(fill = Density), color = "black") + 
    labs(fill = "Population density in 2023 \n (people per sq mile)") +
    coord_map() + 
    theme_void() +  
    scale_fill_viridis() 
```

Remember that the original plot classified densities into our own pre-determined bins before plotting - this might look better!

```{r}
density_polygon <- density_polygon |>
  mutate(Density_intervals = cut(Density, n = 8,
          breaks = c(0, 10, 20, 50, 100, 200, 500, 1000, Inf)))

density_polygon |>
  ggplot(mapping = aes(x = long, y = lat, group = group)) + 
    geom_polygon(aes(fill = Density_intervals), color = "white",
                 linetype = 2) + 
    labs(fill = "Population Density (per sq mile)") +
    coord_map() + 
    theme_void() +  
    scale_fill_brewer(palette = "YlOrRd") 
```

We could even create a static plot using `geom_sf()` using `density_sf`:

```{r}
density_sf <- density_sf |>
  mutate(Density_intervals = cut(Density, n = 8,
          breaks = c(0, 10, 20, 50, 100, 200, 500, 1000, Inf))) 

ggplot(data = density_sf) + 
  geom_sf(aes(fill = Density_intervals), colour = "white", linetype = 2) + 
  theme_void() +  
  scale_fill_brewer(palette = "YlOrRd") 
```

But... why not make an interactive plot instead?

```{r}
density_sf <- density_sf |>
  mutate(labels = str_c(name, ": ", Density, " people per sq mile in 2023"))

labels <- lapply(density_sf$labels, HTML)
pal <- colorNumeric("YlOrRd", density_sf$Density)

leaflet(density_sf) |>
  setView(-96, 37.8, 4) |>
  addTiles() |>
  addPolygons(
    weight = 2,
    opacity = 1,
    color = ~ pal(density_sf$Density),
    dashArray = "3",
    fillOpacity = 0.7,
    highlightOptions = highlightOptions(
      weight = 5,
      color = "#666",
      dashArray = "",
      fillOpacity = 0.7,
      bringToFront = TRUE),
    label = labels,
    labelOptions = labelOptions(
      style = list("font-weight" = "normal", padding = "3px 8px"),
      textsize = "15px",
      direction = "auto")) 
# should use addLegend() but not trivial without pre-set bins
```

Here's an interactive plot with our own bins:

```{r}
#| warning: false

# Create our own category bins for population densities
#   and assign the yellow-orange-red color palette
bins <- c(0, 10, 20, 50, 100, 200, 500, 1000, Inf)
pal <- colorBin("YlOrRd", domain = density_sf$Density, bins = bins)

# Create labels that pop up when we hover over a state.  The labels must
#   be part of a list where each entry is tagged as HTML code.
density_sf <- density_sf |>
  mutate(labels = str_c(name, ": ", Density, " people / sq mile"))
labels <- lapply(density_sf$labels, HTML)

# If want more HTML formatting, use these lines instead of those above:
# states <- states |>
#   mutate(labels = glue("<strong>{name}</strong><br/>{density} people / 
#   mi<sup>2</sup>"))
# labels <- lapply(states$labels, HTML)

leaflet(density_sf) |>
  setView(-96, 37.8, 4) |>
  addTiles() |>
  addPolygons(
    fillColor = ~pal(Density),
    weight = 2,
    opacity = 1,
    color = "white",
    dashArray = "3",
    fillOpacity = 0.7,
    highlightOptions = highlightOptions(
      weight = 5,
      color = "#666",
      dashArray = "",
      fillOpacity = 0.7,
      bringToFront = TRUE),
    label = labels,
    labelOptions = labelOptions(
      style = list("font-weight" = "normal", padding = "3px 8px"),
      textsize = "15px",
      direction = "auto")) |>
  addLegend(pal = pal, values = ~Density, opacity = 0.7, title = NULL,
    position = "bottomright")
```


### On Your Own

1. Use the `rvest` package and `html_table` to read in the table of data found at the link [here](https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population) and create a scatterplot of land area versus the 2022 estimated population.  I give you some starter code below; fill in the "???" and be sure you can explain what EVERY line of code does and why it's necessary.

#| eval: FALSE

city_pop <- read_html("https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population")

pop <- html_nodes(???, ???)
html_table(pop, header = TRUE, fill = TRUE)  # find right table
pop2 <- html_table(pop, header = TRUE, fill = TRUE)[[???]]
pop2

# perform the steps above with the polite package
session <- bow("https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population", force = TRUE)

result <- scrape(session) |>
  html_nodes(???) |>
  html_table(header = TRUE, fill = TRUE)
pop2 <- result[[???]]
pop2

pop3 <- as_tibble(pop2[,c(1:6,8)]) |>
  slice(???) |>
  rename(`State` = `ST`,
         `Estimate2023` = `2023estimate`,
         `Census` = `2020census`,
         `Area` = `2020 land area`,
         `Density` = `2020 density`) |>
  mutate(Estimate2023 = parse_number(Estimate2023),
         Census = parse_number(Census),
         Change = ???   # get rid of % but preserve +/-,
         Area = parse_number(Area),
         Density = parse_number(Density)) |> 
  mutate(City = str_replace(City, "\\[.*$", ""))
pop3

# pick out unusual points
outliers <- pop3 |> 
  filter(Estimate2023 > ??? | Area > ???)

# This will work if don't turn variables from chr to dbl, but in that 
#  case notice how axes are just evenly spaced categorical variables
ggplot(pop3, aes(x = ???, y = ???)) +
  geom_point()  +
  geom_smooth() +
  ggrepel::geom_label_repel(data = ???, aes(label = ???))


2. We would like to create a tibble with 4 years of data (2001-2004) from the Minnesota Wild hockey team.  Specifically, we are interested in the "Scoring Regular Season" table from [this webpage](https://www.hockey-reference.com/teams/MIN/2001.html) and the similar webpages from 2002, 2003, and 2004.  Your final tibble should have 6 columns:  player, year, age, pos (position), gp (games played), and pts (points).

You should (a) write a function called `hockey_stats` with inputs for team and year to scrape data from the "scoring Regular Season" table, and (b) use iteration techniques to scrape and combine 4 years worth of data.  Here are some functions you might consider:

- `row_to_names(row_number = 1)` from the `janitor` package
- `clean_names()` also from the `janitor` package
- `bow()` and `scrape()` from the `polite` package
- `str_c()` from the `stringr` package (for creating urls with user inputs)
- `map2()` and `list_rbind()` for iterating and combining years

Try following these steps:

1) Be sure you can find and clean the correct table from the 2021 season.

```{r}
# Step 0: Check that scraping is allowed
robotstxt::paths_allowed("https://www.hockey-reference.com/teams/MIN/2001.html")

# Step 1: read_html()
hockey_page <- read_html("https://www.hockey-reference.com/teams/MIN/2001.html")

# Step 2: html_nodes()
tables <- html_nodes(hockey_page, css = "table") 
tables  # have to guesstimate which table contains our desired info

# Step 3: html_table()
html_table(tables, header = TRUE, fill = TRUE)    # find the right table
hockey_table <- html_table(tables, header = TRUE, fill = TRUE)[[1]]  
hockey_table
```


2) Organize your `rvest` code from (1) into functions from the `polite` package.

```{r}
session <- bow("https://www.hockey-reference.com/teams/MIN/2001.html", force = TRUE)

result <- scrape(session) |>
  html_nodes(css = "table") |> 
  html_table(header = TRUE, fill = TRUE)
hockey_table <- result[[1]]
hockey_table
```

3) Place the code from (2) into a function where the user can input a team and year.  You would then adjust the url accordingly and produce a clean table for the user.

```{r}
hockey_stats <- function(team, year){
  base_front_url <- "https://www.hockey-reference.com/teams/"
  url <- str_c(base_front_url, team, "/", year, ".html")
  session <- bow(url, force = TRUE)

  result <- scrape(session) |>
    html_nodes(css = "table") |> 
    html_table(header = TRUE, fill = TRUE)
  hockey_table <- result[[1]]
  hockey_table
}
```

```{r}
hockey_stats("MIN", "2001")
```

4) Use `map2` and `list_rbind` to build one data set containing Minnesota Wild data from 2001-2004.

```{r}
specific_years <- c("2001","2002","2003","2004")
mn_hockey_data <- map2("MIN", specific_years, hockey_stats) |> 
  list_rbind()
```


You can download this .qmd file from [here](https://github.com/joeroith/264_spring_2025/blob/main/09_web_scraping.qmd).  Just hit the Download Raw File button.

Credit to Brianna Heggeseth and Leslie Myint from Macalester College for a few of these descriptions and examples.

```{r}
#| include: FALSE

library(tidyverse)
library(stringr)
library(rvest)
library(polite)
library(sf)
library(maps)
library(viridis)
library(leaflet)
library(htmltools)

# Starter step: install SelectorGadget (https://selectorgadget.com/) in your browser

```


# Using rvest for web scraping

Please see `08_table_scraping.qmd` for a preview of web scraping techniques when no API exists, along with ethical considerations when scraping data.  In this file, we will turn to scenarios when the webpage contains data of interest, but it is not already in table form.  


## Recall the four steps to scraping data with functions in the `rvest` library:

0. `robotstxt::paths_allowed()` Check if the website allows scraping, and then make sure we scrape "politely"
1. `read_html()`.  Input the URL containing the data and turn the html code into an XML file (another markup format that's easier to work with).
2. `html_nodes()`.  Extract specific nodes from the XML file by using the CSS path that leads to the content of interest. (use css="table" for tables.)
3. `html_text()`.  Extract content of interest from nodes.  Might also use `html_table()` etc.


## More scraping ethics

### `robots.txt`

`robots.txt` is a file that some websites will publish to clarify what can and cannot be scraped and other constraints about scraping. When a website publishes this file, this we need to comply with the information in it for moral and legal reasons.

We will look through the information in [this tutorial](https://www.zenrows.com/blog/robots-txt-web-scraping) and apply this to the [NIH robots.txt file](https://www.nih.gov/robots.txt).

From our investigation of the NIH `robots.txt`, we learn:

- `User-agent: *`: Anyone is allowed to scrape
- `Crawl-delay: 2`: Need to wait 2 seconds between each page scraped
- No `Visit-time` entry: no restrictions on time of day that scraping is allowed
- No `Request-rate` entry: no restrictions on simultaneous requests
- No mention of `?page=`, `news-events`, `news-releases`, or `https://science.education.nih.gov/` in the `Disallow` sections. (This is what we want to scrape today.)

### robotstxt package

We can also use functions from the [`robotstxt` package](https://cran.r-project.org/web/packages/robotstxt/), which was built to download and parse robots.txt files ([more info](https://cran.r-project.org/web/packages/robotstxt/vignettes/using_robotstxt.html)).  Specifically, the `paths_allowed()` function can check if a bot has permission to access certain pages.


## A timeout to preview some technical ideas

### HTML structure

HTML (hypertext markup language) is the formatting language used to create webpages. We can see the core parts of HTML from the [rvest vignette](https://cran.r-project.org/web/packages/rvest/vignettes/rvest.html).

### Finding CSS Selectors

In order to gather information from a webpage, we must learn the language used to identify patterns of specific information. For example, on the [NIH News Releases page](https://www.nih.gov/news-events/news-releases), we can see that the data is represented in a consistent pattern of image + title + abstract.

We will identify data in a web page using a pattern matching language called [CSS Selectors](https://css-tricks.com/how-css-selectors-work/) that can refer to specific patterns in HTML, the language used to write web pages.

For example:

- Selecting by tag:
    - `"a"` selects all hyperlinks in a webpage ("a" represents "anchor" links in HTML)
    - `"p"` selects all paragraph elements

- Selecting by ID and class:
    - `".description"` selects all elements with `class` equal to "description"
        - The `.` at the beginning is what signifies `class` selection.
        - This is one of the most common CSS selectors for scraping because in HTML, the `class` attribute is extremely commonly used to format webpage elements. (Any number of HTML elements can have the same `class`, which is not true for the `id` attribute.)
    - `"#mainTitle"` selects the SINGLE element with **id** equal to "mainTitle"
        - The `#` at the beginning is what signifies `id` selection.

```html
<p class="title">Title of resource 1</p>
<p class="description">Description of resource 1</p>

<p class="title">Title of resource 2</p>
<p class="description">Description of resource 2</p>
```

**Warning**: Websites change often! So if you are going to scrape a lot of data, it is probably worthwhile to save and date a copy of the website. Otherwise, you may return after some time and your scraping code will include all of the wrong CSS selectors.

### SelectorGadget

Although you can [learn how to use CSS Selectors by hand]([CSS Selectors](https://css-tricks.com/how-css-selectors-work/)), we will use a shortcut by installing the [Selector Gadget](http://selectorgadget.com/) tool.

- There is a version available for Chrome--add it to Chrome via the [Chome Web Store](https://chromewebstore.google.com/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb).
    - Make sure to pin the extension to the menu bar. (Click the 3 dots > Extensions > Manage extensions. Click the "Details" button under SelectorGadget and toggle the "Pin to toolbar" option.)
- There is also a version that can be saved as a bookmark in the browser--see [here](https://rvest.tidyverse.org/articles/selectorgadget.html).

You might watch the Selector Gadget [tutorial video](http://selectorgadget.com/).


## Case Study: NIH News Releases

Our goal is to build a data frame with the article title, publication date, and abstract text for the 50 most recent NIH news releases.  

Head over to the [NIH News Releases page](https://www.nih.gov/news-events/news-releases). Click the Selector Gadget extension icon or bookmark button. As you mouse over the webpage, different parts will be highlighted in orange. Click on the title (but not the live link portion!) of the first news release. You'll notice that the Selector Gadget information in the lower right describes what you clicked on.  (If SelectorGadget ever highlights too much in green, you can click on portions that you do not want to turn them red.)

Scroll through the page to verify that only the information you intend (the description paragraph) is selected. The selector panel shows the CSS selector (`.teaser-title`) and the number of matches for that CSS selector (10). (You may have to be careful with your clicking--there are two overlapping boxes, and clicking on the link of the title can lead to the CSS selector of "a".)


**[Pause to Ponder:]** Repeat the process above to find the correct selectors for the following fields. Make sure that each matches 10 results:

- The publication date

> .date-display-single

```{r}

```


- The article abstract paragraph (which will also include the publication date)

> .teaser-description


### Retrieving Data Using `rvest` and CSS Selectors

Now that we have identified CSS selectors for the information we need, let's fetch the data using the `rvest` package similarly to our approach in `08_table_scraping.qmd`.

```{r}
# check that scraping is allowed (Step 0)
robotstxt::paths_allowed("https://www.nih.gov/news-events/news-releases")

# Step 1: Download the HTML and turn it into an XML file with read_html()
nih <- read_html("https://www.nih.gov/news-events/news-releases")
```

Finding the exact node (e.g. ".teaser-title") is the tricky part.  Among all the html code used to produce a webpage, where do you go to grab the content of interest?  This is where SelectorGadget comes to the rescue!

```{r}
# Step 2: Extract specific nodes with html_nodes()
title_temp <- html_nodes(nih, ".teaser-title")
title_temp

# Step 3: Extract content from nodes with html_text(), html_name(), 
#    html_attrs(), html_children(), html_table(), etc.
# Usually will still need to do some stringr adjustments
title_vec <- html_text(title_temp)
title_vec
```

You can also write this altogether with a pipe:

```{r}
robotstxt::paths_allowed("https://www.nih.gov/news-events/news-releases")

read_html("https://www.nih.gov/news-events/news-releases") |>
  html_nodes(".teaser-title") |>
  html_text()
```

And finally we wrap the 4 steps above into the `bow` and `scrape` functions from the `polite` package:

```{r}
session <- bow("https://www.nih.gov/news-events/news-releases", force = TRUE)

nih_title <- scrape(session) |>
  html_nodes(".teaser-title") |>
  html_text()
nih_title
```


### Putting multiple columns of data together.

Now repeat the process above to extract the publication date and the abstract.

```{r}
nih_pubdate <- scrape(session) |>
  html_nodes(".date-display-single") |>
  html_text()
nih_pubdate

nih_description <- scrape(session) |>
  html_nodes(".teaser-description") |>
  html_text()
nih_description
```

Combine these extracted variables into a single tibble.  Make sure the variables are formatted correctly - e.g. `pubdate` has `date` type, `description` does not contain the `pubdate`, etc.

```{r}
# use tibble() to put multiple columns together into a tibble
nih_top10 <- tibble(title = nih_title, 
                    pubdate = nih_pubdate, 
                    description = nih_description)
nih_top10

# now clean the data
nih_top10 <- nih_top10 |>
  mutate(pubdate = mdy(pubdate),
         description = str_trim(str_replace(description, ".*\\n", "")))
nih_top10
```

NOW - continue this process to build a tibble with the most recent 50 NIH news releases, which will require that you iterate over 5 webpages!  You should write at least one function, and you will need iteration--use both a `for` loop and appropriate `map_()` functions from `purrr`. Some additional hints:

- Mouse over the page buttons at the very bottom of the news home page to see what the URLs look like.
- Include `Sys.sleep(2)` in your function to respect the `Crawl-delay: 2` in the NIH `robots.txt` file.
- Recall that `bind_rows()` from `dplyr` takes a list of data frames and stacks them on top of each other.

**[Pause to Ponder:]** Create a function to scrape a single NIH press release page by filling missing pieces labeled `???`:

```{r}

# Helper function to reduce html_nodes() |> html_text() code duplication
get_text_from_page <- function(page, css_selector) {
  page |>
    html_nodes(css_selector) |>
    html_text()
}

# Main function to scrape and tidy desired attributes
scrape_page <- function(url) {
    Sys.sleep(2)
    page <- read_html(url)
    article_titles <- get_text_from_page(page, ".teaser-title")
    article_dates <- get_text_from_page(page, ".date-display-single")
    article_dates <- mdy(article_dates)
    article_description <- get_text_from_page(page, ".teaser-description")
    article_description <- str_trim(str_replace(article_description, 
                                                ".*\\n", 
                                                "")
                                    )
    
    tibble(
      title = article_titles,
      dates = article_dates,
      description = article_description
    )
}

scrape_page("https://www.nih.gov/news-events/news-releases")
```


**[Pause to Ponder:]** Use a for loop over the first 5 pages:

```{r}

pages <- vector("list", length = 6)
pos <- 0

for (i in 2025:2024) {
  for (j in 0:2) {
    pos <- pos + 1
    url <- str_c("https://www.nih.gov/news-events/news-releases?", i,
                 "&page=", j, "&1=")
    pages[[pos]] <- scrape_page(url)
  }
}

df_articles <- bind_rows(pages)
head(df_articles)
```


**[Pause to Ponder:]** Use map functions in the purrr package:

```{r}

library(purrr)

base_url <- "https://www.nih.gov/news-events/news-releases?page="
urls_all_pages <- str_c(base_url, seq(0,5))

pages2 <- purrr::map(urls_all_pages, scrape_page)
df_articles2 <- bind_rows(pages2)
head(df_articles2)
```
